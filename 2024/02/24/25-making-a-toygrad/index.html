<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Making a Toy Gradient Descent Implementation | Kevin Hu&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="1 IntroductionI’ve recently came across a few of Andrej Karpathy‘s video tutorial series on Machine Learning and I found them immensely fun and educational. I highly appreciate his hands-on approach t">
<meta property="og:type" content="article">
<meta property="og:title" content="Making a Toy Gradient Descent Implementation">
<meta property="og:url" content="https://blog.kevinhu.me/2024/02/24/25-making-a-toygrad/index.html">
<meta property="og:site_name" content="Kevin Hu&#39;s Blog">
<meta property="og:description" content="1 IntroductionI’ve recently came across a few of Andrej Karpathy‘s video tutorial series on Machine Learning and I found them immensely fun and educational. I highly appreciate his hands-on approach t">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://blog.kevinhu.me/2024/02/24/25-making-a-toygrad/input_predict_chart.png">
<meta property="og:image" content="https://raw.githubusercontent.com/karpathy/micrograd/c911406e5ace8742e5841a7e0df113ecb5d54685/gout.svg">
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/5/54/Feed_forward_neural_net.gif">
<meta property="article:published_time" content="2024-02-24T22:34:23.000Z">
<meta property="article:modified_time" content="2024-09-04T05:56:25.284Z">
<meta property="article:author" content="Kevin Hu">
<meta property="article:tag" content="Learning">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog.kevinhu.me/2024/02/24/25-making-a-toygrad/input_predict_chart.png">
<meta name="twitter:creator" content="@Oldgunix">
  
    <link rel="alternative" href="/atom.xml" title="Kevin Hu&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
<link rel="stylesheet" href="/css/style.css">

  


    <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-8ENJJHKTMX"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-8ENJJHKTMX');
  </script>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Kevin Hu&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">A Hungry Fool</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/About/">About</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://blog.kevinhu.me"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-25-making-a-toygrad" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2024/02/24/25-making-a-toygrad/" class="article-date">
  <time datetime="2024-02-24T22:34:23.000Z" itemprop="datePublished">02 24 2024</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Learning/">Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Making a Toy Gradient Descent Implementation
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>I’ve recently came across a few of <a href="https://karpathy.ai/">Andrej Karpathy</a>‘s
<a href="https://www.youtube.com/@AndrejKarpathy">video tutorial series</a>
on Machine Learning and I found them immensely fun and educational.
I highly appreciate his hands-on approach to teaching basic concepts of Machine Learning.</p>
<p>Richard Feynman once famously stated that “What I cannot create, I do not understand.”
So here’s my attempt to create a toy implementation of gradient descent,
to better understand the core algorithm that powers Deep Learning
after learning from by Karpathy’s <a href="https://www.youtube.com/@AndrejKarpathy">video tutorial of micrograd</a>:</p>
<p>https:&#x2F;&#x2F;github.com&#x2F;hxy9243&#x2F;toygrad</p>
<p>Even though there’s a plethora of books, blogs, and references that explains the gradient descent algorithm,
it’s a totally different experience when you get to build it yourself from the scratch.
During this course I found there are quite a few knowledge gaps for myself, things that I’ve taken for granted
and didn’t really fully understand.</p>
<p>And this blog post is my notes during this experience. Even writing this post helped my understanding in many ways.</p>
<p><img src="/2024/02/24/25-making-a-toygrad/input_predict_chart.png" alt="Training and inference example using toygrad"></p>
<span id="more"></span>



<h1 id="2-Key-Concepts"><a href="#2-Key-Concepts" class="headerlink" title="2 Key Concepts"></a>2 Key Concepts</h1><h2 id="2-1-Chain-Rule"><a href="#2-1-Chain-Rule" class="headerlink" title="2.1 Chain Rule"></a>2.1 Chain Rule</h2><p>In Calculus, the chain rule of derivative shows the basic rules for finding the derivatives
of the composite functions.</p>
<p>As we learned from Calculus class, the chain rule states:</p>
<p>$$
\frac{df(g(x))}{dx} &#x3D; \frac{df}{dg} \cdot \frac{dg}{dx} 
$$</p>
<p>For example, the derivative of the function $f(x) &#x3D; sin x^2$ is:</p>
<p>$$
\frac{dsin(x^2)}{dx} &#x3D; \frac{dsin(x^2)}{dx} \cdot \frac{dx^2}{x} &#x3D; cos(x^2) \cdot 2x
$$</p>
<p>With chain-rule, we could derive the auto-grad algorithm to implement backpropagation
on complex compute graphs.</p>
<h2 id="2-2-Backpropagation"><a href="#2-2-Backpropagation" class="headerlink" title="2.2 Backpropagation"></a>2.2 Backpropagation</h2><p>Backpropagation, or backward propagation of errors, is the algorithm to find the derivative
of the loss function (which is a function that computes the difference between prediction and the actual output data).</p>
<p>It calcuates the gradient backwards through the feed-foward network from the last layer to the first.</p>
<p>There are 4 steps to the Backpropagation algorithm:</p>
<ul>
<li><strong>Forward Pass</strong>: where the input data is fed through the model (e.g. a Deep Neural Network) and get the prediction output.</li>
<li><strong>Loss Computation</strong>: where the prediction output is compared with the actual fed data with a function
(e.g. Mean Squred Error, or Cross Entropy Loss). This is the loss function that we’ll aim to minimize ($loss &#x3D; J(w)$).</li>
<li><strong>Backward Pass</strong>: this is where gradient descent comes in.
In this step, we need to find the derivative of the loss function at each parameter ($\frac{\partial J}{\partial wn}$).
Instead of deriving a formula for the derivative of the loss function, we can then apply the chain rule to derive the gradient descent algorithm so as to find the gradients of all parameters backward the computational graph.</li>
<li><strong>Update Parameters</strong>: After getting the gradients for each parameter, we update all parameters by substracting the gradient timed by
a small value that we call the learning rate.</li>
</ul>
<p>And we repeat this 4-step process until the loss is close to the minimal value.</p>
<p>Conceptually, the gradients represent the slope rate at the level of the current parameters. So we could substract the parameters at
the direction of the gradient by a small amount (decided by the learning rate). It’s a process of moving the loss function closer
to the minimal value, at a speed defined by learning rate.</p>
<p>There are a lot of tricks and optimizations to adjusting the learning rate, but that’s outside the scope of this discussion.</p>
<h2 id="2-3-AutoGrad"><a href="#2-3-AutoGrad" class="headerlink" title="2.3 AutoGrad"></a>2.3 AutoGrad</h2><p>AutoGrad, or Automatic Differentiation, is the core of the backpropagation algorithm in the backward step.
It computes the gradients of all parameters in a reverse manner, calculating the derivative of all
parameters of the function by applying the gradient backwards in the compute graph.</p>
<p>And here’s the explanation why the AutoGrad algorithm makes sense.</p>
<p>https:&#x2F;&#x2F;pytorch.org&#x2F;tutorials&#x2F;beginner&#x2F;blitz&#x2F;autograd_tutorial.html#differentiation-in-autograd</p>
<p><img src="https://raw.githubusercontent.com/karpathy/micrograd/c911406e5ace8742e5841a7e0df113ecb5d54685/gout.svg" alt="Autograd explanation, from Karpathy&#39;s tutorial"></p>
<p>The AutoGrad algorithm could be derived by the chain rule. The chain rule in Calculus states that:</p>
<p>$$
\frac{\partial y}{\partial x} &#x3D; \frac{\partial y}{\partial w1} \cdot \frac{\partial w1}{\partial x}
&#x3D; (\frac{\partial y}{\partial w2} \cdot \frac{\partial w2}{\partial w1}) \cdot \frac{\partial w1}{\partial x}
&#x3D; ((\frac{\partial y}{\partial w3} \cdot \frac{\partial w3}{\partial w2}) \cdot \frac{\partial w2}{\partial w1}) \cdot \frac{\partial w1}{\partial x}
$$</p>
<p>Assuming in this simple case, w3 is the result of a computation from w2, i.e. the successor of w2 in the compute graph,
we can derive that:</p>
<p>$$
\frac{\partial y}{\partial w2} &#x3D; \frac{\partial y}{\partial w3} \cdot \frac{\partial w3}{\partial w2}
$$</p>
<p>When we need to find the gradients of all the parameters, we just need to apply this chain rule backward the computational graph. The gradient of each intermediate variables, denoted as:</p>
<p>$$ \bar w_{i} &#x3D; \frac{\partial{y}}{\partial w_{i}} $$</p>
<p>would be the sum of all gradients of its successors in the operation.</p>
<p>With this chain rule in mind, we could start with the final result of the equation and work upward the compute graph.
It has a seed gradient value of 1 (as $\frac{\partial y}{\partial y} &#x3D; 1$), and back-propagate the gradients back to all intermediate parameters.</p>
<p>See more at: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Automatic_differentiation#Reverse_accumulation</p>
<h1 id="3-ToyGrad-Architecture"><a href="#3-ToyGrad-Architecture" class="headerlink" title="3 ToyGrad Architecture"></a>3 ToyGrad Architecture</h1><h2 id="3-1-Value-Engine-Design"><a href="#3-1-Value-Engine-Design" class="headerlink" title="3.1 Value Engine Design"></a>3.1 Value Engine Design</h2><p>The core of this autograd engine design is the <code>Value</code> class that could both express the feed-forward computation
as well as the backward propagation of gradients. These are the steps when considering its design:</p>
<ul>
<li>Basic arithmatics: overriding the basic arithmatics of a Value.</li>
<li>Feed-forward computation: compute the basic elements, overriding the operators of the <code>Value</code> class.</li>
<li>The gradient computation: the basic feedfoward computation and its corresponding backward computation. Each backward
  computation is defined by the derivative of each operation.</li>
</ul>
<p>For example, in the case of multiplication:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Value</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, val</span>):</span><br><span class="line">        <span class="variable language_">self</span>.val = val</span><br><span class="line">        <span class="variable language_">self</span>.grad = <span class="number">0</span></span><br><span class="line">        <span class="variable language_">self</span>.operands = <span class="built_in">set</span>()</span><br><span class="line">        <span class="variable language_">self</span>._backward = <span class="keyword">lambda</span>: <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__mul__</span>(<span class="params">self, other</span>):</span><br><span class="line">        other = Value(other) <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(other, Value) <span class="keyword">else</span> other</span><br><span class="line"></span><br><span class="line">        <span class="comment"># new returned value is the computated value of the operator</span></span><br><span class="line">        v = Value(<span class="variable language_">self</span>.val * other.val)</span><br><span class="line">        v.operands = <span class="built_in">set</span>((<span class="variable language_">self</span>, other))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># here we update the gradient of each parameter from the gradient of the result value</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">_backward</span>():</span><br><span class="line">            <span class="variable language_">self</span>.grad += other.val * v.grad</span><br><span class="line">            other.grad += <span class="variable language_">self</span>.val * v.grad</span><br><span class="line"></span><br><span class="line">        v._backward = _backward</span><br><span class="line">        <span class="keyword">return</span> v</span><br><span class="line"></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>

<p>By calling <code>backward()</code> on the final result of the compute graph, we first assign a gradient of 1 to the final value,
and compute the gradients of all operands of the final value. We can do it in two steps:</p>
<ul>
<li>Create a compute order by reversely traversing the compute graph (a reverse topological sort).</li>
<li>Apply the <code>backward()</code> function on each one of the values in the compute order, thus updating the gradients of all parameters.</li>
</ul>
<h2 id="3-2-Neural-Network-Design"><a href="#3-2-Neural-Network-Design" class="headerlink" title="3.2 Neural Network Design"></a>3.2 Neural Network Design</h2><p>With the Value engine designed, we could now chain these value computation to form a feed-forward neural network
with dense layers, where all Values are connected to the next layer’s values.</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/5/54/Feed_forward_neural_net.gif" alt="Example Neural Network, form Wikipedia"></p>
<p>We’ll also need code for:</p>
<ul>
<li><strong>Linear Dense Layer</strong>: a dense layer connects to all weights from previous layer, added by a bias: $y &#x3D; W \cdot X + b$.</li>
<li><strong>Neural network</strong>: create the layers and the whole neural network. Each layer could be defined as the matrix multiplication of the weights and previous layer added by bias. We can then call the <code>forward()</code> and <code>backward()</code> step on the neural network after feeding it the training input and output data.</li>
<li><strong>Parameter update</strong>: for each training iteration step, update all the parameters. This is usually done by the optimizer in a real implementation.</li>
<li><strong>Training process</strong>: we glue everything together in the training step implementation.</li>
</ul>
<p>Here’s an outline of the code that describes the steps to a model definition and training process:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(<span class="title class_ inherited__">Module</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_features, output_features</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.layer1 = Linear(input_features, <span class="number">8</span>)</span><br><span class="line">        <span class="variable language_">self</span>.layer2 = Linear(<span class="number">8</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="variable language_">self</span>.output = Linear(<span class="number">4</span>, output_features)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        X = <span class="variable language_">self</span>.layer1(X)</span><br><span class="line">        X = [xi.relu() <span class="keyword">for</span> xi <span class="keyword">in</span> X]</span><br><span class="line">        X = <span class="variable language_">self</span>.layer2(X)</span><br><span class="line">        X = [xi.relu() <span class="keyword">for</span> xi <span class="keyword">in</span> X]</span><br><span class="line">        output = <span class="variable language_">self</span>.output(X)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, X, y, epochs=<span class="number">1</span>, learning_rate=<span class="number">1e-5</span></span>):</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">            <span class="comment"># loss = 0.0</span></span><br><span class="line">            final_cost = Value(<span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># get cost function</span></span><br><span class="line">            <span class="keyword">for</span> i, xval <span class="keyword">in</span> <span class="built_in">enumerate</span>(X):</span><br><span class="line">                out = <span class="variable language_">self</span>.forward(xval)</span><br><span class="line">                cost = ((y[i] - out) ** <span class="number">2</span>)[<span class="number">0</span>]</span><br><span class="line">                final_cost += cost</span><br><span class="line"></span><br><span class="line">            final_cost = final_cost / <span class="built_in">len</span>(X)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># backward</span></span><br><span class="line">            final_cost.backward()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># apply the grad with learning rate</span></span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> <span class="variable language_">self</span>.parameters():</span><br><span class="line">                p.val -= p.grad * learning_rate</span><br><span class="line"></span><br><span class="line">            <span class="comment"># zero out the grads</span></span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> <span class="variable language_">self</span>.parameters():</span><br><span class="line">                p.grad = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">            loss = final_cost.val</span><br><span class="line"></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>

<p>If you reached this far, I highly recommend the <a href="https://www.youtube.com/@AndrejKarpathy">video tutorial of micrograd</a>
from Andrej Karpathy himself, along with <a href="https://github.com/karpathy/micrograd">his implementation</a>.
He explains it way better than I could.</p>
<p>Also let me know if you found any problems in this blog post or my version of implementation:</p>
<p>https:&#x2F;&#x2F;github.com&#x2F;hxy9243&#x2F;toygrad</p>
<h2 id="3-3-Things-to-Notice"><a href="#3-3-Things-to-Notice" class="headerlink" title="3.3 Things to Notice"></a>3.3 Things to Notice</h2><ul>
<li>Parameters include weights of the neural network and bias.</li>
<li>Init all the parameters with random values instead of zero.</li>
<li>You need to clean up all the gradients for each iteration of backpropagation. It’s easy to forget this step.</li>
</ul>
<h1 id="4-References"><a href="#4-References" class="headerlink" title="4 References"></a>4 References</h1><ul>
<li>https:&#x2F;&#x2F;www.britannica.com&#x2F;science&#x2F;differentiation-mathematics</li>
<li>https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Derivative</li>
<li>https:&#x2F;&#x2F;machinelearningmastery.com&#x2F;difference-between-backpropagation-and-stochastic-gradient-descent&#x2F;</li>
<li>https:&#x2F;&#x2F;github.com&#x2F;karpathy&#x2F;micrograd</li>
<li>https:&#x2F;&#x2F;pytorch.org&#x2F;tutorials&#x2F;beginner&#x2F;blitz&#x2F;autograd_tutorial.html</li>
<li>https:&#x2F;&#x2F;deepai.org&#x2F;machine-learning-glossary-and-terms&#x2F;backpropagation</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://blog.kevinhu.me/2024/02/24/25-making-a-toygrad/" data-id="cm0nhgtla004nmjeygmypbjx8" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/" rel="tag">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Learning/" rel="tag">Learning</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2025/06/22/Agentic-Programming/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Learning AI Agent Programming (with DSPy)
        
      </div>
    </a>
  
  
    <a href="/2023/09/03/03-Paper-Readings-on-LLM-Task-Performing-ii/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Paper Readings on LLM Task Performing - II</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
    <div class="widget-wrap">
        <h3 class="widget-title">Follow Me</h3>
        <div class="widget">
            <a href="https://twitter.com/Oldgunix?ref_src=twsrc%5Etfw" class="twitter-follow-button"
                data-show-count="false">Follow @Oldgunix</a>
            <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
        </div>
    </div>
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/08/22/Building-a-School-Library-Assistant-with-FastMCP-and-DSPy-II/">Building a School Library Assistant with FastMCP and DSPy (II)</a>
          </li>
        
          <li>
            <a href="/2025/08/09/Building-MCP-with-DSPy/">Building a School Library Assistant with FastMCP and DSPy (I)</a>
          </li>
        
          <li>
            <a href="/2025/06/22/Agentic-Programming/">Learning AI Agent Programming (with DSPy)</a>
          </li>
        
          <li>
            <a href="/2024/02/24/25-making-a-toygrad/">Making a Toy Gradient Descent Implementation</a>
          </li>
        
          <li>
            <a href="/2023/09/03/03-Paper-Readings-on-LLM-Task-Performing-ii/">Paper Readings on LLM Task Performing - II</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/BookReview/">BookReview</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Coding/">Coding</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/ComputerSystem/">ComputerSystem</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Golang/">Golang</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Learning/">Learning</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/">Paper</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/PaperReading/">PaperReading</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Programming/">Programming</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/ProgrammingLanguage/">ProgrammingLanguage</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Reading/">Reading</a><span class="category-list-count">25</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/AI/" style="font-size: 17.5px;">AI</a> <a href="/tags/API/" style="font-size: 10px;">API</a> <a href="/tags/Algorithm/" style="font-size: 10px;">Algorithm</a> <a href="/tags/Aurora/" style="font-size: 10px;">Aurora</a> <a href="/tags/BitHacks/" style="font-size: 10px;">BitHacks</a> <a href="/tags/BookReview/" style="font-size: 16.25px;">BookReview</a> <a href="/tags/Borg/" style="font-size: 10px;">Borg</a> <a href="/tags/C/" style="font-size: 10px;">C</a> <a href="/tags/C/" style="font-size: 10px;">C++</a> <a href="/tags/C-C/" style="font-size: 11.25px;">C/C++</a> <a href="/tags/Cassandra/" style="font-size: 12.5px;">Cassandra</a> <a href="/tags/Channel/" style="font-size: 11.25px;">Channel</a> <a href="/tags/ChatGPT/" style="font-size: 12.5px;">ChatGPT</a> <a href="/tags/Coding/" style="font-size: 10px;">Coding</a> <a href="/tags/Compilers/" style="font-size: 11.25px;">Compilers</a> <a href="/tags/Concurrency/" style="font-size: 10px;">Concurrency</a> <a href="/tags/Data-Science/" style="font-size: 10px;">Data Science</a> <a href="/tags/Database/" style="font-size: 13.75px;">Database</a> <a href="/tags/Datacenter/" style="font-size: 10px;">Datacenter</a> <a href="/tags/Debug/" style="font-size: 10px;">Debug</a> <a href="/tags/DistributedSystems/" style="font-size: 15px;">DistributedSystems</a> <a href="/tags/Docker/" style="font-size: 10px;">Docker</a> <a href="/tags/FPGA/" style="font-size: 10px;">FPGA</a> <a href="/tags/Golang/" style="font-size: 13.75px;">Golang</a> <a href="/tags/HPC/" style="font-size: 10px;">HPC</a> <a href="/tags/Hugepage/" style="font-size: 10px;">Hugepage</a> <a href="/tags/Internet/" style="font-size: 10px;">Internet</a> <a href="/tags/Julia/" style="font-size: 10px;">Julia</a> <a href="/tags/Kafka/" style="font-size: 10px;">Kafka</a> <a href="/tags/LLM/" style="font-size: 12.5px;">LLM</a> <a href="/tags/Learning/" style="font-size: 20px;">Learning</a> <a href="/tags/Linux/" style="font-size: 12.5px;">Linux</a> <a href="/tags/Log/" style="font-size: 10px;">Log</a> <a href="/tags/Machine-Learning/" style="font-size: 10px;">Machine Learning</a> <a href="/tags/Mesos/" style="font-size: 10px;">Mesos</a> <a href="/tags/Multithread/" style="font-size: 10px;">Multithread</a> <a href="/tags/Mutex/" style="font-size: 10px;">Mutex</a> <a href="/tags/NLP/" style="font-size: 12.5px;">NLP</a> <a href="/tags/NoSQL/" style="font-size: 12.5px;">NoSQL</a> <a href="/tags/Paper/" style="font-size: 10px;">Paper</a> <a href="/tags/PaperReading/" style="font-size: 18.75px;">PaperReading</a> <a href="/tags/Programming/" style="font-size: 12.5px;">Programming</a> <a href="/tags/ProgrammingLanguage/" style="font-size: 11.25px;">ProgrammingLanguage</a> <a href="/tags/Python/" style="font-size: 12.5px;">Python</a> <a href="/tags/Ray/" style="font-size: 10px;">Ray</a> <a href="/tags/Reading/" style="font-size: 17.5px;">Reading</a> <a href="/tags/Social/" style="font-size: 10px;">Social</a> <a href="/tags/Sociology/" style="font-size: 10px;">Sociology</a> <a href="/tags/Technology/" style="font-size: 12.5px;">Technology</a> <a href="/tags/Undefined-Behavior/" style="font-size: 11.25px;">Undefined Behavior</a> <a href="/tags/WeeklyPaper/" style="font-size: 10px;">WeeklyPaper</a> <a href="/tags/WorldWideWeb/" style="font-size: 10px;">WorldWideWeb</a> <a href="/tags/Zookeeper/" style="font-size: 10px;">Zookeeper</a> <a href="/tags/infrastructure/" style="font-size: 10px;">infrastructure</a> <a href="/tags/openAPI/" style="font-size: 10px;">openAPI</a> <a href="/tags/systems/" style="font-size: 10px;">systems</a> <a href="/tags/tracing/" style="font-size: 10px;">tracing</a> <a href="/tags/web/" style="font-size: 10px;">web</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/08/">August 2025</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/06/">June 2025</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/02/">February 2024</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/09/">September 2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/09/">September 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/08/">August 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">May 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">November 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">July 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">April 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">July 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">June 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">November 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">September 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">August 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">May 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/02/">February 2016</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/12/">December 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/11/">November 2015</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/10/">October 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/04/">April 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/03/">March 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/01/">January 2015</a><span class="archive-list-count">2</span></li></ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2025 Kevin Hu<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/About/" class="mobile-nav-link">About</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>


  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>